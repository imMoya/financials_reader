{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e0f336c-e428-4734-bc2a-0dea39445981",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, time\n",
    "import pyspark\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "af1c4cbd-8df7-4cfe-b6e6-5e07cefc19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DataType, BooleanType, NullType, IntegerType, StringType, MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28504334-dcf5-455d-8f4c-af4324e37809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, explode, to_date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f312f63-d4d0-411e-acc3-dd91b9c4f496",
   "metadata": {},
   "source": [
    "# Getting file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "617297fd-c3a8-41ef-b85b-3f6530080ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = 'archive'\n",
    "dir_list = []\n",
    "dir_list += [os.path.join(DIR,file) for file in os.listdir(DIR) if os.path.isdir(os.path.join(DIR, file))]\n",
    "dir_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3586fcc7-8f21-44d4-bb7e-29da75e3a6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get latest quarter\n",
    "quarter_folder = dir_list[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b66cc132-c087-4fae-b2c5-1ae2c70ea7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = []\n",
    "json_files += [os.path.join(quarter_folder, file) for file in os.listdir(quarter_folder)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72ae4c33-bff6-424c-bf57-5078de13e75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'archive/2022.QTR2/0001493152-22-013349.json'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file = json_files[0]\n",
    "json_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf194c3-3a47-444d-9836-28483556e24e",
   "metadata": {},
   "source": [
    "# Initialize PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73e15d52-e5da-479a-9f5c-194a8f6e4795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/23 21:08:03 WARN Utils: Your hostname, Ignacios-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.230 instead (on interface en0)\n",
      "23/01/23 21:08:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "23/01/23 21:08:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/01/23 21:08:03 WARN DependencyUtils: Local jar /Users/ignaciomoyaredondo/Documents/data/pyspark_financials/postgresql-42.5.1.jar does not exist, skipping.\n",
      "23/01/23 21:08:04 INFO SparkContext: Running Spark version 3.3.1\n",
      "23/01/23 21:08:04 INFO ResourceUtils: ==============================================================\n",
      "23/01/23 21:08:04 INFO ResourceUtils: No custom resources configured for spark.driver.\n",
      "23/01/23 21:08:04 INFO ResourceUtils: ==============================================================\n",
      "23/01/23 21:08:04 INFO SparkContext: Submitted application: Decode_json_files\n",
      "23/01/23 21:08:04 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\n",
      "23/01/23 21:08:04 INFO ResourceProfile: Limiting resource is cpu\n",
      "23/01/23 21:08:04 INFO ResourceProfileManager: Added ResourceProfile id: 0\n",
      "23/01/23 21:08:04 INFO SecurityManager: Changing view acls to: ignaciomoyaredondo\n",
      "23/01/23 21:08:04 INFO SecurityManager: Changing modify acls to: ignaciomoyaredondo\n",
      "23/01/23 21:08:04 INFO SecurityManager: Changing view acls groups to: \n",
      "23/01/23 21:08:04 INFO SecurityManager: Changing modify acls groups to: \n",
      "23/01/23 21:08:04 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ignaciomoyaredondo); groups with view permissions: Set(); users  with modify permissions: Set(ignaciomoyaredondo); groups with modify permissions: Set()\n",
      "23/01/23 21:08:04 INFO Utils: Successfully started service 'sparkDriver' on port 49850.\n",
      "23/01/23 21:08:04 INFO SparkEnv: Registering MapOutputTracker\n",
      "23/01/23 21:08:04 INFO SparkEnv: Registering BlockManagerMaster\n",
      "23/01/23 21:08:04 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\n",
      "23/01/23 21:08:04 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\n",
      "23/01/23 21:08:04 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n",
      "23/01/23 21:08:04 INFO DiskBlockManager: Created local directory at /private/var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/blockmgr-cc4a9752-3b0f-4de4-bde5-ea92493e3eac\n",
      "23/01/23 21:08:04 INFO MemoryStore: MemoryStore started with capacity 366.3 MiB\n",
      "23/01/23 21:08:04 INFO SparkEnv: Registering OutputCommitCoordinator\n",
      "23/01/23 21:08:04 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/01/23 21:08:04 INFO Utils: Successfully started service 'SparkUI' on port 4041.\n",
      "23/01/23 21:08:04 ERROR SparkContext: Failed to add postgresql-42.5.1.jar to Spark environment\n",
      "java.io.FileNotFoundException: Jar /Users/ignaciomoyaredondo/Documents/data/pyspark_financials/postgresql-42.5.1.jar not found\n",
      "\tat org.apache.spark.SparkContext.addLocalJarFile$1(SparkContext.scala:1949)\n",
      "\tat org.apache.spark.SparkContext.addJar(SparkContext.scala:2004)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12(SparkContext.scala:507)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$new$12$adapted(SparkContext.scala:507)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:507)\n",
      "\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n",
      "\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n",
      "\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:238)\n",
      "\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\n",
      "\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "23/01/23 21:08:05 INFO Executor: Starting executor ID driver on host 192.168.1.230\n",
      "23/01/23 21:08:05 INFO Executor: Starting executor with user classpath (userClassPathFirst = false): ''\n",
      "23/01/23 21:08:05 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 49853.\n",
      "23/01/23 21:08:05 INFO NettyBlockTransferService: Server created on 192.168.1.230:49853\n",
      "23/01/23 21:08:05 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\n",
      "23/01/23 21:08:05 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 192.168.1.230, 49853, None)\n",
      "23/01/23 21:08:05 INFO BlockManagerMasterEndpoint: Registering block manager 192.168.1.230:49853 with 366.3 MiB RAM, BlockManagerId(driver, 192.168.1.230, 49853, None)\n",
      "23/01/23 21:08:05 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 192.168.1.230, 49853, None)\n",
      "23/01/23 21:08:05 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 192.168.1.230, 49853, None)\n"
     ]
    }
   ],
   "source": [
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"Decode_json_files\") \\\n",
    "    .config(\"spark.jars\", \"postgresql-42.5.1.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f3a3e4-b9e0-43e8-86fc-e677b9cea6f4",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "702e0477-e8db-4788-9beb-648a1a259385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 5.25 µs\n",
      "23/01/23 21:08:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "23/01/23 21:08:05 INFO SharedState: Warehouse path is 'file:/Users/ignaciomoyaredondo/Documents/data/pyspark_financials/spark-warehouse'.\n",
      "23/01/23 21:08:06 INFO InMemoryFileIndex: It took 24 ms to list leaf files for 1 paths.\n",
      "23/01/23 21:08:06 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 356.0 KiB, free 366.0 MiB)\n",
      "23/01/23 21:08:06 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 34.2 KiB, free 365.9 MiB)\n",
      "23/01/23 21:08:06 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.1.230:49853 (size: 34.2 KiB, free: 366.3 MiB)\n",
      "23/01/23 21:08:06 INFO SparkContext: Created broadcast 0 from json at NativeMethodAccessorImpl.java:0\n",
      "23/01/23 21:08:06 INFO FileInputFormat: Total input files to process : 1\n",
      "23/01/23 21:08:06 INFO FileInputFormat: Total input files to process : 1\n",
      "23/01/23 21:08:06 INFO SparkContext: Starting job: json at NativeMethodAccessorImpl.java:0\n",
      "23/01/23 21:08:06 INFO DAGScheduler: Got job 0 (json at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/23 21:08:06 INFO DAGScheduler: Final stage: ResultStage 0 (json at NativeMethodAccessorImpl.java:0)\n",
      "23/01/23 21:08:06 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/23 21:08:06 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/23 21:08:06 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at json at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/23 21:08:07 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 9.0 KiB, free 365.9 MiB)\n",
      "23/01/23 21:08:07 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.8 KiB, free 365.9 MiB)\n",
      "23/01/23 21:08:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.1.230:49853 (size: 4.8 KiB, free: 366.3 MiB)\n",
      "23/01/23 21:08:07 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/23 21:08:07 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at json at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/23 21:08:07 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0\n",
      "23/01/23 21:08:07 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (192.168.1.230, executor driver, partition 0, PROCESS_LOCAL, 4656 bytes) taskResourceAssignments Map()\n",
      "23/01/23 21:08:07 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)\n",
      "23/01/23 21:08:07 INFO BinaryFileRDD: Input split: Paths:/Users/ignaciomoyaredondo/Documents/data/pyspark_financials/archive/2022.QTR2/0001493152-22-013349.json:0+12735\n",
      "23/01/23 21:08:07 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 2385 bytes result sent to driver\n",
      "23/01/23 21:08:07 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 405 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/23 21:08:07 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "23/01/23 21:08:07 INFO DAGScheduler: ResultStage 0 (json at NativeMethodAccessorImpl.java:0) finished in 0.515 s\n",
      "23/01/23 21:08:07 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/23 21:08:07 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished\n",
      "23/01/23 21:08:07 INFO DAGScheduler: Job 0 finished: json at NativeMethodAccessorImpl.java:0, took 0.563244 s\n",
      "23/01/23 21:08:08 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 192.168.1.230:49853 in memory (size: 4.8 KiB, free: 366.3 MiB)\n",
      "23/01/23 21:08:08 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.1.230:49853 in memory (size: 34.2 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "df = spark.read.option(\"multiline\", \"true\").json(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63ff6867-d32f-4b4a-9655-f4a5bed48374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- bs: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- concept: string (nullable = true)\n",
      " |    |    |    |-- label: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |-- cf: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- concept: string (nullable = true)\n",
      " |    |    |    |-- label: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |-- ic: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- concept: string (nullable = true)\n",
      " |    |    |    |-- label: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |-- endDate: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- startDate: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f890b6cf-a8f4-413e-a512-32079f2b38e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/23 21:13:28 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/23 21:13:28 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/01/23 21:13:28 INFO FileSourceStrategy: Output Data Schema: struct<data: struct<bs: array<struct<concept:string,label:string,unit:string,value:string>>, cf: array<struct<concept:string,label:string,unit:string,value:string>>, ic: array<struct<concept:string,label:string,unit:string,value:string>> ... 1 more fields>, endDate: string, quarter: string, startDate: string, symbol: string ... 1 more field>\n",
      "23/01/23 21:13:28 INFO CodeGenerator: Code generated in 249.525346 ms\n",
      "23/01/23 21:13:28 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 350.3 KiB, free 366.0 MiB)\n",
      "23/01/23 21:13:28 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.9 MiB)\n",
      "23/01/23 21:13:28 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.230:49853 (size: 34.1 KiB, free: 366.3 MiB)\n",
      "23/01/23 21:13:28 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/23 21:13:28 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/23 21:13:28 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/23 21:13:28 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/23 21:13:28 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/23 21:13:28 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/23 21:13:28 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/23 21:13:28 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/23 21:13:28 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 25.7 KiB, free 365.9 MiB)\n",
      "23/01/23 21:13:28 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 365.9 MiB)\n",
      "23/01/23 21:13:28 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.230:49853 (size: 9.6 KiB, free: 366.3 MiB)\n",
      "23/01/23 21:13:28 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/23 21:13:28 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/23 21:13:28 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "23/01/23 21:13:29 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.230, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()\n",
      "23/01/23 21:13:29 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "23/01/23 21:13:29 INFO FileScanRDD: Reading File path: file:///Users/ignaciomoyaredondo/Documents/data/pyspark_financials/archive/2022.QTR2/0001493152-22-013349.json, range: 0-12735, partition values: [empty row]\n",
      "23/01/23 21:13:29 INFO CodeGenerator: Code generated in 31.986301 ms\n",
      "23/01/23 21:13:29 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 3585 bytes result sent to driver\n",
      "23/01/23 21:13:29 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 145 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/23 21:13:29 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/01/23 21:13:29 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.168 s\n",
      "23/01/23 21:13:29 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/23 21:13:29 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished\n",
      "23/01/23 21:13:29 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.173703 s\n",
      "23/01/23 21:13:29 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.230:49853 in memory (size: 9.6 KiB, free: 366.3 MiB)\n",
      "23/01/23 21:13:29 INFO CodeGenerator: Code generated in 16.131077 ms\n",
      "+--------------------+----------+-------+----------+------+----+\n",
      "|                data|   endDate|quarter| startDate|symbol|year|\n",
      "+--------------------+----------+-------+----------+------+----+\n",
      "|{[{AssetsCurrent,...|2022-03-31|     Q1|2022-01-01|  OGEN|2022|\n",
      "+--------------------+----------+-------+----------+------+----+\n",
      "\n",
      "23/01/23 21:38:05 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.230:49853 in memory (size: 34.1 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf1c10-9be0-4de9-9b3a-bb1e8aacdee1",
   "metadata": {},
   "source": [
    "### Balance sheet spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3d9fe05b-c058-485e-ae87-0d2ade67dded",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bs = df.withColumn(\"bs\", explode('data.bs')).select('bs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dbc8d1e-d555-48e4-91c6-1b781be4f58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bs: struct (nullable = true)\n",
      " |    |-- concept: string (nullable = true)\n",
      " |    |-- label: string (nullable = true)\n",
      " |    |-- unit: string (nullable = true)\n",
      " |    |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bs.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "887627ce-6972-4fcf-9f02-735fd729ca1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bs = df_bs.select('bs.concept', 'bs.label', 'bs.unit', 'bs.value').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f7e13fa-1e7b-4178-9a28-33f3deb6ddd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 19:34:10 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/17 19:34:10 INFO FileSourceStrategy: Post-Scan Filters: (size(data#0.bs, true) > 0),isnotnull(data#0.bs)\n",
      "23/01/17 19:34:10 INFO FileSourceStrategy: Output Data Schema: struct<data: struct<bs: array<struct<concept:string,label:string,unit:string,value:string>>, cf: array<struct<concept:string,label:string,unit:string,value:string>>, ic: array<struct<concept:string,label:string,unit:string,value:string>> ... 1 more fields>>\n",
      "23/01/17 19:34:10 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/01/17 19:34:10 INFO CodeGenerator: Code generated in 210.625113 ms\n",
      "23/01/17 19:34:10 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 350.3 KiB, free 366.0 MiB)\n",
      "23/01/17 19:34:10 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.9 MiB)\n",
      "23/01/17 19:34:10 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.1.230:54172 (size: 34.1 KiB, free: 366.3 MiB)\n",
      "23/01/17 19:34:10 INFO SparkContext: Created broadcast 2 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:10 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/17 19:34:10 INFO DAGScheduler: Registering RDD 6 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "23/01/17 19:34:10 INFO DAGScheduler: Got map stage job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/17 19:34:10 INFO DAGScheduler: Final stage: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/17 19:34:10 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/17 19:34:10 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:10 INFO DAGScheduler: Submitting ShuffleMapStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/17 19:34:10 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 48.2 KiB, free 365.9 MiB)\n",
      "23/01/17 19:34:10 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 365.9 MiB)\n",
      "23/01/17 19:34:10 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 192.168.1.230:54172 (size: 19.6 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:10 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:10 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:10 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:10 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (192.168.1.230, executor driver, partition 0, PROCESS_LOCAL, 4970 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:10 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)\n",
      "23/01/17 19:34:10 INFO CodeGenerator: Code generated in 34.900607 ms\n",
      "23/01/17 19:34:10 INFO CodeGenerator: Code generated in 6.194773 ms\n",
      "23/01/17 19:34:11 INFO CodeGenerator: Code generated in 7.563023 ms\n",
      "23/01/17 19:34:11 INFO FileScanRDD: Reading File path: file:///Users/ignaciomoyaredondo/Documents/data/pyspark_financials/archive/2022.QTR2/0001493152-22-013349.json, range: 0-12735, partition values: [empty row]\n",
      "23/01/17 19:34:11 INFO CodeGenerator: Code generated in 21.185869 ms\n",
      "23/01/17 19:34:11 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 2883 bytes result sent to driver\n",
      "23/01/17 19:34:11 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 251 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:11 INFO DAGScheduler: ShuffleMapStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.272 s\n",
      "23/01/17 19:34:11 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/01/17 19:34:11 INFO DAGScheduler: running: Set()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: waiting: Set()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: failed: Set()\n",
      "23/01/17 19:34:11 INFO ShufflePartitionsUtil: For shuffle(0), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/01/17 19:34:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/01/17 19:34:11 INFO CodeGenerator: Code generated in 20.472364 ms\n",
      "23/01/17 19:34:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Got job 2 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Final stage: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 2)\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Submitting ResultStage 3 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 42.2 KiB, free 365.8 MiB)\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 18.8 KiB, free 365.8 MiB)\n",
      "23/01/17 19:34:11 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 192.168.1.230:54172 (size: 18.8 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:11 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 3 (MapPartitionsRDD[9] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Adding task set 3.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:11 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 2) (192.168.1.230, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:11 INFO Executor: Running task 0.0 in stage 3.0 (TID 2)\n",
      "23/01/17 19:34:11 INFO ShuffleBlockFetcherIterator: Getting 1 (2.4 KiB) non-empty blocks including 1 (2.4 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/01/17 19:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 10 ms\n",
      "23/01/17 19:34:11 INFO Executor: Finished task 0.0 in stage 3.0 (TID 2). 5027 bytes result sent to driver\n",
      "23/01/17 19:34:11 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 2) in 76 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:11 INFO DAGScheduler: ResultStage 3 (showString at NativeMethodAccessorImpl.java:0) finished in 0.088 s\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Job 2 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 3: Stage finished\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Job 2 finished: showString at NativeMethodAccessorImpl.java:0, took 0.103395 s\n",
      "23/01/17 19:34:11 INFO CodeGenerator: Code generated in 10.884382 ms\n",
      "+--------------------+--------------------+----+----------+\n",
      "|             concept|               label|unit|     value|\n",
      "+--------------------+--------------------+----+----------+\n",
      "|LiabilitiesNoncur...|Total long-term l...| usd|    253928|\n",
      "|       AssetsCurrent|Total current assets| usd|  21649936|\n",
      "| NotesPayableCurrent|                    | usd|    122175|\n",
      "|RetainedEarningsA...|                    | usd|-177309522|\n",
      "|AdditionalPaidInC...|                    | usd| 195077466|\n",
      "|LiabilitiesAndSto...|Total liabilities...| usd|  22203742|\n",
      "|OtherReceivablesN...|                    | usd|       N/A|\n",
      "| PreferredStockValue|                    | usd|   2656713|\n",
      "|CashAndCashEquiva...|                    | usd|  21372463|\n",
      "|              Assets|        Total assets| usd|  22203742|\n",
      "|  LiabilitiesCurrent|Total current lia...| usd|   1408762|\n",
      "|OperatingLeaseLia...|Operating lease l...| usd|    253928|\n",
      "|OperatingLeaseLia...|                    | usd|    192535|\n",
      "|    CommonStockValue|                    | usd|    116395|\n",
      "|OperatingLeaseRig...|                    | usd|    431961|\n",
      "|AccountsPayableAn...|                    | usd|   1094052|\n",
      "|PrepaidExpenseAnd...|                    | usd|    277473|\n",
      "|PropertyPlantAndE...|                    | usd|    121845|\n",
      "|  StockholdersEquity|Total shareholder...| usd|  20541052|\n",
      "+--------------------+--------------------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_bs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eaed97-24da-4479-9d29-4be342c0cdc5",
   "metadata": {},
   "source": [
    "### Cash flow spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e6c062-0c2d-4f74-867a-8d4e02bd4a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cf = df.withColumn(\"bs\", explode('data.cf')).select('bs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee47db4-f723-4329-924d-f5accc53b71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- bs: struct (nullable = true)\n",
      " |    |-- concept: string (nullable = true)\n",
      " |    |-- label: string (nullable = true)\n",
      " |    |-- unit: string (nullable = true)\n",
      " |    |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1c07f79-b8db-46b0-97d7-6fef2101fe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cf = df_cf.select('bs.concept', 'bs.label', 'bs.unit', 'bs.value').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f6ba47b-5635-4db8-9503-1f154136e7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 19:34:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/17 19:34:11 INFO FileSourceStrategy: Post-Scan Filters: (size(data#0.cf, true) > 0),isnotnull(data#0.cf)\n",
      "23/01/17 19:34:11 INFO FileSourceStrategy: Output Data Schema: struct<data: struct<bs: array<struct<concept:string,label:string,unit:string,value:string>>, cf: array<struct<concept:string,label:string,unit:string,value:string>>, ic: array<struct<concept:string,label:string,unit:string,value:string>> ... 1 more fields>>\n",
      "23/01/17 19:34:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/01/17 19:34:11 INFO CodeGenerator: Code generated in 33.387662 ms\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 350.3 KiB, free 365.5 MiB)\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 365.4 MiB)\n",
      "23/01/17 19:34:11 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 192.168.1.230:54172 (size: 34.1 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:11 INFO SparkContext: Created broadcast 5 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:11 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Registering RDD 13 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Got map stage job 3 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Final stage: ShuffleMapStage 4 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Submitting ShuffleMapStage 4 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 48.1 KiB, free 365.4 MiB)\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 365.4 MiB)\n",
      "23/01/17 19:34:11 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 192.168.1.230:54172 (size: 19.6 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:11 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 4 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:11 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (192.168.1.230, executor driver, partition 0, PROCESS_LOCAL, 4970 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:11 INFO Executor: Running task 0.0 in stage 4.0 (TID 3)\n",
      "23/01/17 19:34:11 INFO FileScanRDD: Reading File path: file:///Users/ignaciomoyaredondo/Documents/data/pyspark_financials/archive/2022.QTR2/0001493152-22-013349.json, range: 0-12735, partition values: [empty row]\n",
      "23/01/17 19:34:11 INFO Executor: Finished task 0.0 in stage 4.0 (TID 3). 2840 bytes result sent to driver\n",
      "23/01/17 19:34:11 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 61 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:11 INFO DAGScheduler: ShuffleMapStage 4 (showString at NativeMethodAccessorImpl.java:0) finished in 0.071 s\n",
      "23/01/17 19:34:11 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/01/17 19:34:11 INFO DAGScheduler: running: Set()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: waiting: Set()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: failed: Set()\n",
      "23/01/17 19:34:11 INFO ShufflePartitionsUtil: For shuffle(1), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/01/17 19:34:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/01/17 19:34:11 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Got job 4 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Final stage: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 42.1 KiB, free 365.3 MiB)\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 365.3 MiB)\n",
      "23/01/17 19:34:11 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 192.168.1.230:54172 (size: 18.7 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:11 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[16] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:11 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 4) (192.168.1.230, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:11 INFO Executor: Running task 0.0 in stage 6.0 (TID 4)\n",
      "23/01/17 19:34:11 INFO ShuffleBlockFetcherIterator: Getting 1 (2.9 KiB) non-empty blocks including 1 (2.9 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/01/17 19:34:11 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/01/17 19:34:11 INFO Executor: Finished task 0.0 in stage 6.0 (TID 4). 5340 bytes result sent to driver\n",
      "23/01/17 19:34:11 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 4) in 17 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:11 INFO DAGScheduler: ResultStage 6 (showString at NativeMethodAccessorImpl.java:0) finished in 0.025 s\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/17 19:34:11 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished\n",
      "23/01/17 19:34:11 INFO DAGScheduler: Job 4 finished: showString at NativeMethodAccessorImpl.java:0, took 0.032115 s\n",
      "+--------------------+--------------------+----+--------+\n",
      "|             concept|               label|unit|   value|\n",
      "+--------------------+--------------------+----+--------+\n",
      "|       NetIncomeLoss|                    | usd|-6035394|\n",
      "|NetCashProvidedBy...|Net cash used in ...| usd|-5636952|\n",
      "|NetCashProvidedBy...|Net cash used in ...| usd|  -75047|\n",
      "|ProceedsFromWarra...|                    | usd|     N/A|\n",
      "|DepreciationAndAm...|                    | usd|    8468|\n",
      "|ProceedsFromIssua...|                    | usd|     N/A|\n",
      "|IncreaseDecreaseI...|Prepaid expenses ...| usd| -157226|\n",
      "|     InterestPaidNet|                    | usd|    3246|\n",
      "|PaymentsForRepurc...|Redemption of Ser...| usd|     N/A|\n",
      "|PaymentsToAcquire...|Purchase of prope...| usd|   87047|\n",
      "|IncreaseDecreaseI...|Accounts payable ...| usd|  146478|\n",
      "|GainLossOnSaleOfP...|Gain on sale of p...| usd|   10964|\n",
      "|ProceedsFromSaleO...|                    | usd|   12000|\n",
      "|RepaymentsOfShort...|Payments on short...| usd|  181241|\n",
      "|OGEN:StockDividen...|                    | usd|     N/A|\n",
      "|IncreaseDecreaseI...|   Other receivables| usd|   -6987|\n",
      "|CashCashEquivalen...|Net increase (dec...| usd|-5893240|\n",
      "|ShareBasedCompens...|                    | usd|   90247|\n",
      "|NetCashProvidedBy...|Net cash provided...| usd| -181241|\n",
      "+--------------------+--------------------+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_cf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ca175-226f-40fb-ae51-122f610e9b80",
   "metadata": {},
   "source": [
    "### Income statement spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca27619c-d792-429d-964b-3be8c0f8d077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic = df.withColumn(\"ic\", explode('data.ic')).select('ic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a56bdba6-3291-45bb-af5a-483062133a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ic: struct (nullable = true)\n",
      " |    |-- concept: string (nullable = true)\n",
      " |    |-- label: string (nullable = true)\n",
      " |    |-- unit: string (nullable = true)\n",
      " |    |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ic.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f99c0847-b996-49bb-9925-4e8c779b2bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ic = df_ic.select('ic.concept', 'ic.label', 'ic.unit', 'ic.value').distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2517bf3a-dec9-4962-b5fe-51a31ea3309b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 19:34:11 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/17 19:34:11 INFO FileSourceStrategy: Post-Scan Filters: (size(data#0.ic, true) > 0),isnotnull(data#0.ic)\n",
      "23/01/17 19:34:11 INFO FileSourceStrategy: Output Data Schema: struct<data: struct<bs: array<struct<concept:string,label:string,unit:string,value:string>>, cf: array<struct<concept:string,label:string,unit:string,value:string>>, ic: array<struct<concept:string,label:string,unit:string,value:string>> ... 1 more fields>>\n",
      "23/01/17 19:34:11 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/01/17 19:34:11 INFO CodeGenerator: Code generated in 33.83924 ms\n",
      "23/01/17 19:34:11 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 350.3 KiB, free 365.0 MiB)\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.9 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 192.168.1.230:54172 (size: 34.1 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO SparkContext: Created broadcast 8 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Registering RDD 20 (showString at NativeMethodAccessorImpl.java:0) as input to shuffle 2\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Got map stage job 5 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Final stage: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 48.1 KiB, free 364.9 MiB)\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 19.6 KiB, free 364.9 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 192.168.1.230:54172 (size: 19.6 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[20] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 5) (192.168.1.230, executor driver, partition 0, PROCESS_LOCAL, 4970 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:12 INFO Executor: Running task 0.0 in stage 7.0 (TID 5)\n",
      "23/01/17 19:34:12 INFO FileScanRDD: Reading File path: file:///Users/ignaciomoyaredondo/Documents/data/pyspark_financials/archive/2022.QTR2/0001493152-22-013349.json, range: 0-12735, partition values: [empty row]\n",
      "23/01/17 19:34:12 INFO Executor: Finished task 0.0 in stage 7.0 (TID 5). 2840 bytes result sent to driver\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 5) in 49 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:12 INFO DAGScheduler: ShuffleMapStage 7 (showString at NativeMethodAccessorImpl.java:0) finished in 0.057 s\n",
      "23/01/17 19:34:12 INFO DAGScheduler: looking for newly runnable stages\n",
      "23/01/17 19:34:12 INFO DAGScheduler: running: Set()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: waiting: Set()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: failed: Set()\n",
      "23/01/17 19:34:12 INFO ShufflePartitionsUtil: For shuffle(2), advisory target size: 67108864, actual target size 1048576, minimum partition size: 1048576\n",
      "23/01/17 19:34:12 INFO HashAggregateExec: spark.sql.codegen.aggregate.map.twolevel.enabled is set to true, but current version of codegened fast hashmap does not support this aggregate.\n",
      "23/01/17 19:34:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Got job 6 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Final stage: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 42.1 KiB, free 364.8 MiB)\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 18.7 KiB, free 364.8 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 192.168.1.230:54172 (size: 18.7 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[23] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 6) (192.168.1.230, executor driver, partition 0, NODE_LOCAL, 4453 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:12 INFO Executor: Running task 0.0 in stage 9.0 (TID 6)\n",
      "23/01/17 19:34:12 INFO ShuffleBlockFetcherIterator: Getting 1 (2.0 KiB) non-empty blocks including 1 (2.0 KiB) local and 0 (0.0 B) host-local and 0 (0.0 B) push-merged-local and 0 (0.0 B) remote blocks\n",
      "23/01/17 19:34:12 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms\n",
      "23/01/17 19:34:12 INFO Executor: Finished task 0.0 in stage 9.0 (TID 6). 4887 bytes result sent to driver\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 6) in 12 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:12 INFO DAGScheduler: ResultStage 9 (showString at NativeMethodAccessorImpl.java:0) finished in 0.018 s\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Job 6 finished: showString at NativeMethodAccessorImpl.java:0, took 0.023434 s\n",
      "+--------------------+--------------------+----------+---------+\n",
      "|             concept|               label|      unit|    value|\n",
      "+--------------------+--------------------+----------+---------+\n",
      "|OGEN:LocalBusines...|  Local business tax|       usd|      490|\n",
      "|IncomeTaxExpenseB...|                    |       usd|      N/A|\n",
      "|GeneralAndAdminis...|                    |       usd|  1331549|\n",
      "| OperatingIncomeLoss|Loss from operations|       usd| -6054528|\n",
      "|OtherNonoperating...|                    |       usd|    10964|\n",
      "|EarningsPerShareB...|                    |usd/shares|    -0.05|\n",
      "|WeightedAverageNu...|                    |    shares|116394806|\n",
      "|RevenueFromContra...|                    |       usd|    15083|\n",
      "|ResearchAndDevelo...|                    |       usd|  4738062|\n",
      "|NonoperatingIncom...|Total other incom...|       usd|    19134|\n",
      "|IncomeLossFromCon...|Loss before incom...|       usd| -6035394|\n",
      "|InvestmentIncomeI...|                    |       usd|    11906|\n",
      "|     InterestExpense|    Interest expense|       usd|     3246|\n",
      "|   OperatingExpenses|Total operating e...|       usd|  6069611|\n",
      "|       NetIncomeLoss|            Net loss|       usd| -6035394|\n",
      "+--------------------+--------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d00fc1a-8d87-4a50-8a20-c9a97b82c844",
   "metadata": {},
   "source": [
    "### General information about the company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c46649a1-6779-4c5f-a69d-ff83b8728b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- data: struct (nullable = true)\n",
      " |    |-- bs: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- concept: string (nullable = true)\n",
      " |    |    |    |-- label: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |-- cf: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- concept: string (nullable = true)\n",
      " |    |    |    |-- label: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |    |-- ic: array (nullable = true)\n",
      " |    |    |-- element: struct (containsNull = true)\n",
      " |    |    |    |-- concept: string (nullable = true)\n",
      " |    |    |    |-- label: string (nullable = true)\n",
      " |    |    |    |-- unit: string (nullable = true)\n",
      " |    |    |    |-- value: string (nullable = true)\n",
      " |-- endDate: string (nullable = true)\n",
      " |-- quarter: string (nullable = true)\n",
      " |-- startDate: string (nullable = true)\n",
      " |-- symbol: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b1446f71-93e9-417d-b4ad-859da8f1ef61",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('startDate', to_date(df.startDate, 'yyyy-MM-dd'))\n",
    "df = df.withColumn('endDate',to_date(df.endDate, 'yyyy-MM-dd'))\n",
    "df = df.withColumn('year', df.year.cast('int'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "37fdd75f-8bca-4d73-9561-3e28e4a1a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 19:34:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/17 19:34:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/01/17 19:34:12 INFO FileSourceStrategy: Output Data Schema: struct<year: string>\n",
      "23/01/17 19:34:12 INFO CodeGenerator: Code generated in 6.940201 ms\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 350.3 KiB, free 364.5 MiB)\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.4 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 192.168.1.230:54172 (size: 34.1 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO SparkContext: Created broadcast 11 from collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1\n",
      "23/01/17 19:34:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/17 19:34:12 INFO SparkContext: Starting job: collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Got job 7 (collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1) with 1 output partitions\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Final stage: ResultStage 10 (collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1)\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[27] at collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1), which has no missing parents\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 12.8 KiB, free 364.4 MiB)\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 6.5 KiB, free 364.4 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 192.168.1.230:54172 (size: 6.5 KiB, free: 366.0 MiB)\n",
      "23/01/17 19:34:12 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[27] at collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 7) (192.168.1.230, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:12 INFO Executor: Running task 0.0 in stage 10.0 (TID 7)\n",
      "23/01/17 19:34:12 INFO FileScanRDD: Reading File path: file:///Users/ignaciomoyaredondo/Documents/data/pyspark_financials/archive/2022.QTR2/0001493152-22-013349.json, range: 0-12735, partition values: [empty row]\n",
      "23/01/17 19:34:12 INFO CodeGenerator: Code generated in 7.077882 ms\n",
      "23/01/17 19:34:12 INFO Executor: Finished task 0.0 in stage 10.0 (TID 7). 1497 bytes result sent to driver\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 7) in 22 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:12 INFO DAGScheduler: ResultStage 10 (collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1) finished in 0.029 s\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 10: Stage finished\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Job 7 finished: collect at /var/folders/zf/p75y8jj52c72dh5cv6ys92pm0000gn/T/ipykernel_86379/2964144282.py:1, took 0.032870 s\n"
     ]
    }
   ],
   "source": [
    "year = df.select('year').collect()[0]#show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b7e3e5e-7c9a-4ba1-bf10-ae685f971a43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 19:34:12 INFO FileSourceStrategy: Pushed Filters: \n",
      "23/01/17 19:34:12 INFO FileSourceStrategy: Post-Scan Filters: \n",
      "23/01/17 19:34:12 INFO FileSourceStrategy: Output Data Schema: struct<data: struct<bs: array<struct<concept:string,label:string,unit:string,value:string>>, cf: array<struct<concept:string,label:string,unit:string,value:string>>, ic: array<struct<concept:string,label:string,unit:string,value:string>> ... 1 more fields>, endDate: string, quarter: string, startDate: string, symbol: string ... 1 more field>\n",
      "23/01/17 19:34:12 INFO CodeGenerator: Code generated in 28.154311 ms\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 350.3 KiB, free 364.1 MiB)\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 34.1 KiB, free 364.0 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 192.168.1.230:54172 (size: 34.1 KiB, free: 366.0 MiB)\n",
      "23/01/17 19:34:12 INFO SparkContext: Created broadcast 13 from showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:12 INFO FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "23/01/17 19:34:12 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Got job 8 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Final stage: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0)\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Parents of final stage: List()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Missing parents: List()\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 29.2 KiB, free 364.0 MiB)\n",
      "23/01/17 19:34:12 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.7 KiB, free 364.0 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 192.168.1.230:54172 (size: 10.7 KiB, free: 366.0 MiB)\n",
      "23/01/17 19:34:12 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[31] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 8) (192.168.1.230, executor driver, partition 0, PROCESS_LOCAL, 4981 bytes) taskResourceAssignments Map()\n",
      "23/01/17 19:34:12 INFO Executor: Running task 0.0 in stage 11.0 (TID 8)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 192.168.1.230:54172 in memory (size: 6.5 KiB, free: 366.0 MiB)\n",
      "23/01/17 19:34:12 INFO FileScanRDD: Reading File path: file:///Users/ignaciomoyaredondo/Documents/data/pyspark_financials/archive/2022.QTR2/0001493152-22-013349.json, range: 0-12735, partition values: [empty row]\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 192.168.1.230:54172 in memory (size: 34.1 KiB, free: 366.0 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 192.168.1.230:54172 in memory (size: 19.6 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 192.168.1.230:54172 in memory (size: 19.6 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 192.168.1.230:54172 in memory (size: 18.8 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 192.168.1.230:54172 in memory (size: 34.1 KiB, free: 366.1 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 192.168.1.230:54172 in memory (size: 34.1 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 192.168.1.230:54172 in memory (size: 19.6 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 192.168.1.230:54172 in memory (size: 18.7 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:12 INFO CodeGenerator: Code generated in 23.197674 ms\n",
      "23/01/17 19:34:12 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 192.168.1.230:54172 in memory (size: 18.7 KiB, free: 366.2 MiB)\n",
      "23/01/17 19:34:12 INFO Executor: Finished task 0.0 in stage 11.0 (TID 8). 3585 bytes result sent to driver\n",
      "23/01/17 19:34:12 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 8) in 56 ms on 192.168.1.230 (executor driver) (1/1)\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool \n",
      "23/01/17 19:34:12 INFO DAGScheduler: ResultStage 11 (showString at NativeMethodAccessorImpl.java:0) finished in 0.074 s\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "23/01/17 19:34:12 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished\n",
      "23/01/17 19:34:12 INFO DAGScheduler: Job 8 finished: showString at NativeMethodAccessorImpl.java:0, took 0.076975 s\n",
      "23/01/17 19:34:12 INFO CodeGenerator: Code generated in 9.707483 ms\n",
      "+--------------------+----------+-------+----------+------+----+\n",
      "|                data|   endDate|quarter| startDate|symbol|year|\n",
      "+--------------------+----------+-------+----------+------+----+\n",
      "|{[{AssetsCurrent,...|2022-03-31|     Q1|2022-01-01|  OGEN|2022|\n",
      "+--------------------+----------+-------+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85548257-5ff3-499b-8ef8-ca8aaa67b6bf",
   "metadata": {},
   "source": [
    "# Track one company over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc059bef-b231-4b09-9dd5-e4499323df87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json_in_quarter(dir_quarter):\n",
    "    json_files = []\n",
    "    json_files += [os.path.join(quarter_folder, file) for file in os.listdir(quarter_folder)]\n",
    "    return json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "357ea084-9ccc-4c2f-872f-1869e1f77f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_in_json_pyspark(json_file):\n",
    "    try:\n",
    "        df = spark.read.option(\"multiline\", \"true\").json(json_file)\n",
    "        return df.select(\"symbol\").collect()[0][0]\n",
    "    except NameError:\n",
    "        print('Please initialize Spark session in variable called \"spark\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bfd390-5240-465e-98a0-e8d0186628e4",
   "metadata": {},
   "source": [
    "Let's get a random ticker so that one can see how "
   ]
  },
  {
   "cell_type": "raw",
   "id": "285406fb-2ede-478b-8d75-0bca1e8a805f",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "tickers = []\n",
    "for json_file in json_files[:100]:\n",
    "    tickers.append(get_ticker_in_json_pyspark(json_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32f95fe8-a247-497a-ba00-3948fc997e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ticker_in_json(json_file):\n",
    "    try:\n",
    "        with open(json_file, 'r') as jf:\n",
    "            df = json.load(jf)\n",
    "        return df['symbol']\n",
    "    except IsADirectoryError:\n",
    "        pass    "
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4b99b49-ce40-478c-991e-c9cc6ff11327",
   "metadata": {},
   "source": [
    "%%timeit\n",
    "tickers = []\n",
    "for json_file in json_files[:100]:\n",
    "    tickers.append(get_ticker_in_json(json_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ffae7d-3ed5-474a-850d-71eb7241ae2d",
   "metadata": {},
   "source": [
    "### Create database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "52567e54-dcf6-4e00-a9a2-8d15a2736b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dfdd7fa4-7e0c-4cce-95c0-39a8fd6a36d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['config.ini']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5492fc33-d953-4300-ab45-b51371ad8ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DB_NAME = config['DB']['name']\n",
    "DB_USER = config['DB']['user']\n",
    "DB_PASS = config['DB']['pass']\n",
    "DB_HOST = config['DB']['host']\n",
    "DB_PORT = config['DB']['port']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69911eaf-b8dd-49ec-951b-ce5f3f2b71c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection to database successful\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    conn = psycopg2.connect(database = DB_NAME, user = DB_USER, \n",
    "                            password = DB_PASS, host = DB_HOST,\n",
    "                            port = DB_PORT)\n",
    "    print(\"Connection to database successful\")\n",
    "except:\n",
    "    print(\"Connection to database failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6033e4d4-617b-4ba3-ac7e-1abcaf2c380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "001c6b80-8931-482e-9b78-7ebedb488aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Get lists to be implemented in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67e0a986-bdf7-4598-b995-81b7d37bbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_year_qtr_of_dir(filedir):\n",
    "    year = int(re.search('archive/(.*).QTR', filedir).group(1))\n",
    "    qtr = int(re.search('QTR(.*)', filedir).group(1))\n",
    "    return year, qtr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "222fbc4a-98bb-4c29-8d82-6553448c9eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = []\n",
    "for json_file in json_files[:100]:\n",
    "    tickers.append(get_ticker_in_json(json_file))\n",
    "tickers_id = list(enumerate(tickers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54620535-9e43-4c8b-ab8a-b2242feb3b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filedir = dir_list[0]\n",
    "year, qtr = get_year_qtr_of_dir(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "80ee03a5-50db-44f8-a1ac-a8b85cc067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = get_json_in_quarter(filedir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a9b7c82-99fc-448e-b2e6-b024e59da4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tickers_id[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28e89800-2297-47b3-b2a9-b9520267e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "            DROP TABLE IF EXISTS summary_archive;\n",
    "            CREATE TABLE IF NOT EXISTS summary_archive (\n",
    "            year INT NOT NULL,\n",
    "            quarter INT NOT NULL, \n",
    "            symbol VARCHAR ( 50 ) NOT NULL,\n",
    "            id INT NOT NULL\n",
    "            )\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a9cbb904-cce2-4a99-b03c-91c723bcbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "            INSERT INTO summary_archive (year, quarter, symbol, id)\n",
    "            VALUES (%s, %s, %s, %s);\n",
    "            \"\"\", \n",
    "            (year, qtr, tickers_id[0][1], tickers_id[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4de5a7c-eac2-43c4-9b86-e35cbf060e21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2009, 2, 'OGEN', 0)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur.execute(\"SELECT * FROM summary_archive\")\n",
    "a = cur.fetchall()\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7e5cb681-d098-44c4-8902-7daf211f26d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226c204e-1b68-422c-837a-8279d059ed4b",
   "metadata": {},
   "source": [
    "#### Automate the summary archive table creation for a given quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54b0420e-cc7d-42f4-95b2-0cff2e3e8227",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute(\"\"\"\n",
    "            DROP TABLE IF EXISTS summary_archive;\n",
    "            CREATE TABLE IF NOT EXISTS summary_archive (\n",
    "            year INT NOT NULL,\n",
    "            quarter INT NOT NULL, \n",
    "            symbol VARCHAR ( 50 ),\n",
    "            id INT\n",
    "            )\n",
    "            \"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3fb9af2-e9f1-4032-a0c8-96be284f09c6",
   "metadata": {},
   "source": [
    "Note that the drop table only has to be executed at the beginning of the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d1ec3609-729e-4200-992b-ae4eb6841254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/01/17 20:04:06 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 192.168.1.230:54172 in memory (size: 34.1 KiB, free: 366.3 MiB)\n",
      "23/01/17 20:04:06 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 192.168.1.230:54172 in memory (size: 10.7 KiB, free: 366.3 MiB)\n",
      "23/01/17 20:04:06 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 192.168.1.230:54172 in memory (size: 34.1 KiB, free: 366.3 MiB)\n"
     ]
    }
   ],
   "source": [
    "for filedir in dir_list[:5]:\n",
    "    json_files = get_json_in_quarter(filedir)\n",
    "    tickers = []\n",
    "    for json_file in json_files:\n",
    "        tickers.append(get_ticker_in_json(json_file))\n",
    "    tickers_id = list(enumerate(tickers))\n",
    "    for ticker in tickers_id:\n",
    "        cur.execute(\"\"\"\n",
    "                INSERT INTO summary_archive (year, quarter, symbol, id)\n",
    "                VALUES (%s, %s, %s, %s);\n",
    "                \"\"\", \n",
    "                (year, qtr, ticker[1], ticker[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3bb249a4-a25d-4cc8-a5b6-64a6e7534822",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2173884-6f71-4197-8cdb-feb68406a0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3603587-5051-4f14-8dbc-dd9a4852b693",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "pyspark-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
